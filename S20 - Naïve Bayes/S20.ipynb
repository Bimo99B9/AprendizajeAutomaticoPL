{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logo de AA1](logo_AA1_texto_small.png) \n",
    "# Sesión 20 - Naïve Bayes\n",
    "\n",
    "Habitualmente tenemos nuestros ejemplos definidos como pares $(x,y)$, donde $x$ es un vector que contiene los valores que presenta un ejemplo para cada uno de sus atributos ($x=(x_1, \\dots, x_d)$) e $y$ se corresponde con la clase de ese ejemplo. Para responder a la pregunta ¿cuál es la probabilidad de que un ejemplo descrito con los valores $x$ sea de la clase $y$? podemos hacer uso de modelos probabilísticos recurriendo al Teorema de Bayes:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Para poder aplicar este teorema en el problema que nos ocupa debemos hacer una asunción fuerte: *los atributos de cualquier ejemplo son independientes entre sí una vez que sabemos a qué clase pertenece el ejemplo*.\n",
    "\n",
    "Realizando esta asunción podemos formular el clasificador **Naïve Bayes**:\n",
    "\n",
    "$$\n",
    "P(y|x_1, \\dots, x_d) = \\frac{P(x_1, \\dots, x_d|y)P(y)}{P(x_1, \\dots, x_d)}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $P(y)$ es la *probabilidad a priori* de que un ejemplo sea de la clase $y$\n",
    "- $P(x_1, \\dots, x_d)$ es la probabilidad de que se dé un ejemplo con los valores $(x_1, \\dots, x_d)$ en los atributos. Esta probabilidad es conocida como la *probabilidad marginal*\n",
    "- $P(x_1, \\dots, x_d|y)$ es la probabilidad de que un ejemplo de la clase $y$ tenga los valores $(x_1, \\dots, x_d)$ en los atributos. Esta probabilidad se conoce como *verosimilitud* (*likelihood*)\n",
    "- $P(y|x_1, \\dots, x_d)$ es la probabilidad de que dado un ejemplo con los valores $(x_1, \\dots, x_d)$ en los atributos sea de la clase $y$. Esta probabilidad es conocida como *probabilidad a posteriori*\n",
    "\n",
    "También debemos hacer otra asunción sobre la distribución que sigue la verosimilitud de cada atributo, $P(x_i|y)$. Se suele trabajar con 3 distribuciones diferentes:\n",
    "1. Normal. Asumimos que los valores de los atributos siguen una campana de Gauss\n",
    "2. Multinomial. Asumimos que los atributos pueden tomar un subconjunto discreto de valores\n",
    "3. Bernoulli. Asumimos que los atributos pueden tomar dos valores discretos\n",
    "\n",
    "`Sckit-learn` tiene implementado un algoritmo para cada una de estas tres asunciones. Nos centraremos en la primera de ellas puesto que es la más utilizada y veremos información de las otras dos aunque no trabajaremos con ellas.\n",
    "\n",
    "## 20.1 `GaussianNB`\n",
    "Como habitualmente trabajamos con valores continuos en los atributos es muy habitual realizar la asunción de que los atributos siguen una distribución normal y eso hace que `GaussianNB` sea el clasificador Naïve Bayes más utilizado.\n",
    "\n",
    "Vamos a cargar un conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################\n",
      "### cargar el conjunto y separar X e y\n",
      "##########################################\n",
      "Clases: ['b' 'g']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atr1</th>\n",
       "      <th>atr2</th>\n",
       "      <th>atr3</th>\n",
       "      <th>atr4</th>\n",
       "      <th>atr5</th>\n",
       "      <th>atr6</th>\n",
       "      <th>atr7</th>\n",
       "      <th>atr8</th>\n",
       "      <th>atr9</th>\n",
       "      <th>atr10</th>\n",
       "      <th>...</th>\n",
       "      <th>atr25</th>\n",
       "      <th>atr26</th>\n",
       "      <th>atr27</th>\n",
       "      <th>atr28</th>\n",
       "      <th>atr29</th>\n",
       "      <th>atr30</th>\n",
       "      <th>atr31</th>\n",
       "      <th>atr32</th>\n",
       "      <th>atr33</th>\n",
       "      <th>atr34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56811</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.20332</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57528</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03286</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83508</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.73739</td>\n",
       "      <td>-0.14706</td>\n",
       "      <td>0.84349</td>\n",
       "      <td>-0.05567</td>\n",
       "      <td>0.90441</td>\n",
       "      <td>-0.04622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95378</td>\n",
       "      <td>-0.04202</td>\n",
       "      <td>0.83479</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.12815</td>\n",
       "      <td>0.86660</td>\n",
       "      <td>-0.10714</td>\n",
       "      <td>0.90546</td>\n",
       "      <td>-0.04307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95113</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.95183</td>\n",
       "      <td>-0.02723</td>\n",
       "      <td>0.93438</td>\n",
       "      <td>-0.01920</td>\n",
       "      <td>0.94590</td>\n",
       "      <td>0.01606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94520</td>\n",
       "      <td>0.01361</td>\n",
       "      <td>0.93522</td>\n",
       "      <td>0.04925</td>\n",
       "      <td>0.93159</td>\n",
       "      <td>0.08168</td>\n",
       "      <td>0.94066</td>\n",
       "      <td>-0.00035</td>\n",
       "      <td>0.91483</td>\n",
       "      <td>0.04712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94701</td>\n",
       "      <td>-0.00034</td>\n",
       "      <td>0.93207</td>\n",
       "      <td>-0.03227</td>\n",
       "      <td>0.95177</td>\n",
       "      <td>-0.03431</td>\n",
       "      <td>0.95584</td>\n",
       "      <td>0.02446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.93988</td>\n",
       "      <td>0.03193</td>\n",
       "      <td>0.92489</td>\n",
       "      <td>0.02542</td>\n",
       "      <td>0.92120</td>\n",
       "      <td>0.02242</td>\n",
       "      <td>0.92459</td>\n",
       "      <td>0.00442</td>\n",
       "      <td>0.92697</td>\n",
       "      <td>-0.00577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90608</td>\n",
       "      <td>-0.01657</td>\n",
       "      <td>0.98122</td>\n",
       "      <td>-0.01989</td>\n",
       "      <td>0.95691</td>\n",
       "      <td>-0.03646</td>\n",
       "      <td>0.85746</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91050</td>\n",
       "      <td>-0.02099</td>\n",
       "      <td>0.89147</td>\n",
       "      <td>-0.07760</td>\n",
       "      <td>0.82983</td>\n",
       "      <td>-0.17238</td>\n",
       "      <td>0.96022</td>\n",
       "      <td>-0.03757</td>\n",
       "      <td>0.87403</td>\n",
       "      <td>-0.16243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84710</td>\n",
       "      <td>0.13533</td>\n",
       "      <td>0.73638</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>0.87873</td>\n",
       "      <td>0.08260</td>\n",
       "      <td>0.88928</td>\n",
       "      <td>-0.09139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.86467</td>\n",
       "      <td>-0.15114</td>\n",
       "      <td>0.81147</td>\n",
       "      <td>-0.04822</td>\n",
       "      <td>0.78207</td>\n",
       "      <td>-0.00703</td>\n",
       "      <td>0.75747</td>\n",
       "      <td>-0.06678</td>\n",
       "      <td>0.85764</td>\n",
       "      <td>-0.06151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     atr1  atr2     atr3     atr4     atr5     atr6     atr7     atr8  \\\n",
       "0       1     0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708   \n",
       "1       1     0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597   \n",
       "2       1     0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062   \n",
       "3       1     0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000   \n",
       "4       1     0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255   \n",
       "..    ...   ...      ...      ...      ...      ...      ...      ...   \n",
       "346     1     0  0.83508  0.08298  0.73739 -0.14706  0.84349 -0.05567   \n",
       "347     1     0  0.95113  0.00419  0.95183 -0.02723  0.93438 -0.01920   \n",
       "348     1     0  0.94701 -0.00034  0.93207 -0.03227  0.95177 -0.03431   \n",
       "349     1     0  0.90608 -0.01657  0.98122 -0.01989  0.95691 -0.03646   \n",
       "350     1     0  0.84710  0.13533  0.73638 -0.06151  0.87873  0.08260   \n",
       "\n",
       "        atr9    atr10  ...    atr25    atr26    atr27    atr28    atr29  \\\n",
       "0    1.00000  0.03760  ...  0.56811 -0.51171  0.41078 -0.46168  0.21266   \n",
       "1    1.00000 -0.04549  ... -0.20332 -0.26569 -0.20468 -0.18401 -0.19040   \n",
       "2    0.88965  0.01198  ...  0.57528 -0.40220  0.58984 -0.22145  0.43100   \n",
       "3    0.00000  0.00000  ...  1.00000  0.90695  0.51613  1.00000  1.00000   \n",
       "4    0.77152 -0.16399  ...  0.03286 -0.65158  0.13290 -0.53206  0.02431   \n",
       "..       ...      ...  ...      ...      ...      ...      ...      ...   \n",
       "346  0.90441 -0.04622  ...  0.95378 -0.04202  0.83479  0.00123  1.00000   \n",
       "347  0.94590  0.01606  ...  0.94520  0.01361  0.93522  0.04925  0.93159   \n",
       "348  0.95584  0.02446  ...  0.93988  0.03193  0.92489  0.02542  0.92120   \n",
       "349  0.85746  0.00110  ...  0.91050 -0.02099  0.89147 -0.07760  0.82983   \n",
       "350  0.88928 -0.09139  ...  0.86467 -0.15114  0.81147 -0.04822  0.78207   \n",
       "\n",
       "       atr30    atr31    atr32    atr33    atr34  \n",
       "0   -0.34090  0.42267 -0.54487  0.18641 -0.45300  \n",
       "1   -0.11593 -0.16626 -0.06288 -0.13738 -0.02447  \n",
       "2   -0.17365  0.60436 -0.24180  0.56045 -0.38238  \n",
       "3   -0.20099  0.25682  1.00000 -0.32382  1.00000  \n",
       "4   -0.62197 -0.05707 -0.59573 -0.04608 -0.65697  \n",
       "..       ...      ...      ...      ...      ...  \n",
       "346  0.12815  0.86660 -0.10714  0.90546 -0.04307  \n",
       "347  0.08168  0.94066 -0.00035  0.91483  0.04712  \n",
       "348  0.02242  0.92459  0.00442  0.92697 -0.00577  \n",
       "349 -0.17238  0.96022 -0.03757  0.87403 -0.16243  \n",
       "350 -0.00703  0.75747 -0.06678  0.85764 -0.06151  \n",
       "\n",
       "[351 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "346    1\n",
       "347    1\n",
       "348    1\n",
       "349    1\n",
       "350    1\n",
       "Name: clase, Length: 351, dtype: int32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedKFold,\n",
    "                                     train_test_split)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### cargar el conjunto y separar X e y')\n",
    "print('##########################################')\n",
    "\n",
    "# se llama a la función read_csv\n",
    "# no tiene missing y las columnas están separadas por ','\n",
    "# tampoco cabecera, así que hay que dar nombre a las columnas (como en el names no vienen indicados creamos nombres)\n",
    "cabecera = ['atr'+str(x) for x in range(1,35)]\n",
    "cabecera.append('clase')\n",
    "df = pd.read_csv('ionosphere.data', names=cabecera)\n",
    "filas, columnas = df.shape\n",
    "\n",
    "# se crea el objeto LabelEncoder para transformar la clase\n",
    "class_enc = LabelEncoder() \n",
    "\n",
    "# se transforma la clase\n",
    "df['clase'] = class_enc.fit_transform(df['clase'])\n",
    "print(\"Clases:\", class_enc.classes_)\n",
    "\n",
    "# separamos los atributos y los almacenamos en X\n",
    "X = df.drop(['clase'], axis=1)\n",
    "display(X)\n",
    "\n",
    "# separamos la clase y la almacenamos en Y\n",
    "y = df['clase']\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos unos ejemplos como conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################\n",
      "### Hold-out 70-30\n",
      "##########################################\n"
     ]
    }
   ],
   "source": [
    "print('\\n##########################################')\n",
    "print('### Hold-out 70-30')\n",
    "print('##########################################')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un clasificador `GaussianNB` y obtenemos su accuracy en el conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8679\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "sys_nb = GaussianNB()\n",
    "sys_nb.fit(X_train, y_train) \n",
    "y_pred = sys_nb.predict(X_test)\n",
    "print(\"Accuracy: %.4f\" % metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GaussianNB` es un algoritmo con muy pocos hiperparámetros: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB \n",
    "\n",
    "Únicamente el hiperparámetro `var_smoothing` puede influir en el rendimiento del sistema. En caso de que influya, las variaciones en el rendimiento no suelen ser muy grandes como ocurre con los hiperparámetros de otros sistemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################\n",
      "### se busca el mejor valor para var_smoothing\n",
      "##########################################\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Accuracy para cada valor: [0.83265306 0.84489796 0.87346939 0.86938776 0.85306122 0.8244898 ]\n",
      "Mejor combinación de hiperparámetros: {'var_smoothing': 1e-06}\n",
      "Mejor rendimiento obtenido en GridSearch: 0.8735\n",
      "Accuracy en el conjunto de test: 0.8585\n"
     ]
    }
   ],
   "source": [
    "print('\\n##########################################')\n",
    "print('### se busca el mejor valor para var_smoothing')\n",
    "print('##########################################')\n",
    "\n",
    "# se definen los valores de var_smoothing que se quieren probar\n",
    "valores_de_smoothing = [1e-1, 1e-3, 1e-6, 1e-9, 1e-12, 1e-20]\n",
    "\n",
    "# se crea un generador de folds estratificados partiendo el conjunto en 5 trozos\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "# creamos la grid search \n",
    "gs_nb = GridSearchCV(sys_nb, param_grid={'var_smoothing':valores_de_smoothing}, scoring='accuracy', cv=folds, verbose=1, n_jobs=-1)\n",
    "\n",
    "# ejecutamos la búsqueda\n",
    "gs_nb_trained = gs_nb.fit(X_train, y_train)\n",
    "\n",
    "# resultados\n",
    "print(\"Accuracy para cada valor:\", gs_nb_trained.cv_results_['mean_test_score'])\n",
    "print(\"Mejor combinación de hiperparámetros:\", gs_nb_trained.best_params_)\n",
    "print(\"Mejor rendimiento obtenido en GridSearch: %.4f\" % gs_nb_trained.best_score_)\n",
    "y_pred = gs_nb_trained.predict(X_test)\n",
    "print(\"Accuracy en el conjunto de test: %.4f\" % metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los resultados obtenidos para cada valor de `var_smoothing` son similares y para valores muy extremos de dicho hiperparámetro no observamos rendimientos escandalosamente malos.\n",
    "\n",
    "Además, también podemos apreciar que el rendimiento sobre el conjunto de test es peor que el que teníamos antes, aunque ahora hayamos seleccionado el mejor valor para el hiperparámetro. Una de las razones para que esto haya sucedido es que puede ser que el conjunto de datos separado para test tenga unas características particulares que provoquen esta situación. Por eso, siempre será mejor evaluar utilizando algún método que garantice que todos los ejemplos aparezcan en el conjunto de test, como sucede cuando utilizamos una validación cruzada o un leave-one-out (aunque a veces no podremos utilizarlos por su coste computacional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.2 `MultinomialNB` y `BernoulliNB`\n",
    "\n",
    "Cuando los atributos son discretos pueden seguir una distribución multinomial o de Bernoulli. Será de Bornoulli si los atributos pueden tomar únicamente dos valores y multinomial si pueden tomar más valores.\n",
    "\n",
    "Estas son situaciones que suelen darse a menudo cuando se trabaja en clasificación de textos, algo que está fuera de los objetivos de esta asignatura y, por tanto, os dejamos cierta información que **NO formará parte de la evaluación de esta asignatura**.\n",
    "\n",
    "Dentro de `Sckit-learn` los algoritmos para trabajar bajo estas asunciones son:\n",
    "- `BernoulliNB` https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "- `MultinomialNB` https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB \n",
    "\n",
    "El campo del análisis de textos es muy amplio y en este grado hay una asignatura exclusiva enfocada en su estudio. Aquí os dejamos unos enlaces por si tenéis curiosidad:\n",
    "- https://en.wikipedia.org/wiki/Text_mining\n",
    "- https://en.wikipedia.org/wiki/Document_classification\n",
    "- https://en.wikipedia.org/wiki/Sentiment_analysis \n",
    "\n",
    "## 20.3 Calibrado de las probabilidades\n",
    "\n",
    "Las probabilidades que genera `GaussianNB` al utilizar `predict_proba()` no son buenas y no pueden ser utilizadas como tales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8679\n",
      "Probabilidades de los 10 primeros ejemplos\n",
      "[[1.00000000e+00 0.00000000e+00]\n",
      " [8.48314852e-13 1.00000000e+00]\n",
      " [2.10942641e-06 9.99997891e-01]\n",
      " [1.97812055e-13 1.00000000e+00]\n",
      " [1.00000000e+00 0.00000000e+00]\n",
      " [9.40487902e-01 5.95120975e-02]\n",
      " [1.00000000e+00 0.00000000e+00]\n",
      " [1.82326080e-13 1.00000000e+00]\n",
      " [5.35288657e-07 9.99999465e-01]\n",
      " [2.68765985e-01 7.31234015e-01]]\n",
      "Orden de las clases: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "sys_nb = GaussianNB()\n",
    "sys_nb.fit(X_train, y_train) \n",
    "y_pred = sys_nb.predict(X_test)\n",
    "print(\"Accuracy: %.4f\" % metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# probabilidades\n",
    "cuantos = 10\n",
    "print(\"Probabilidades de los %d primeros ejemplos\" % cuantos)\n",
    "print(sys_nb.predict_proba(X_test.iloc[0:cuantos]))\n",
    "print(\"Orden de las clases:\", sys_nb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, las probabilidades obtenidas tienden a ser valores extremos muy cercanos a 0 o a 1. \n",
    "\n",
    "Estas probabilidades sí son útiles para establecer un ranking, es decir, ordenan bastante bien a los ejemplos en cuanto a su compatibilidad con cada clase. Sin embargo, no son útiles si se quiere ofrecer una predicción acompañada de una probabilidad: \"*este ejemplo tiene una probabilidad del 78.3% de ser de la clase 1*\". \n",
    "\n",
    "Si queremos obtener probabilidades que tengan sentido debemos utilizar la clase `CalibratedClassifierCV`, que utilizando una validación cruzada de k folds realiza un calibrado de las probabilidades para que se ajusten lo más posible a la realidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8679\n",
      "Probabilidades de los 10 primeros ejemplos\n",
      "[[0.88706279 0.11293721]\n",
      " [0.14068252 0.85931748]\n",
      " [0.14068419 0.85931581]\n",
      " [0.14068252 0.85931748]\n",
      " [0.88706279 0.11293721]\n",
      " [0.74401302 0.25598698]\n",
      " [0.88706279 0.11293721]\n",
      " [0.14068252 0.85931748]\n",
      " [0.14068293 0.85931707]\n",
      " [0.38253915 0.61746085]]\n",
      "Orden de las clases: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "sys_nb = GaussianNB()\n",
    "\n",
    "# se crea una instancia de CalibratedClassifierCV que utiliza el generador de fold creado anteriormente\n",
    "sys_nb_calib = CalibratedClassifierCV(sys_nb, method='sigmoid', cv=folds)\n",
    "\n",
    "# se entrena y se evalúa\n",
    "sys_nb_calib.fit(X_train, y_train) \n",
    "y_pred = sys_nb_calib.predict(X_test)\n",
    "print(\"Accuracy: %.4f\" % metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# probabilidades\n",
    "cuantos = 10\n",
    "print(\"Probabilidades de los %d primeros ejemplos\" % cuantos)\n",
    "print(sys_nb_calib.predict_proba(X_test.iloc[0:cuantos]))\n",
    "print(\"Orden de las clases:\", sys_nb_calib.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que las probabilidades obtenidas después de la calibración son más realistas. En este caso la accuracy coincide antes y después de calibrar, pero no tiene por qué ser así y, en ocasiones, el rendimiento puede cambiar considerablemente.\n",
    "\n",
    "En el código anterior hemos creado una instancia de `CalibratedClassifierCV` que utiliza el *Platt scaling* (`method='sigmoid'`) para la calibración de las probabilidades. Si recordáis, este método es el que se utiliza para el cálculo de las probabilidades con las SVM.\n",
    "\n",
    "En https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html?highlight=calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV podéis obtener más información.\n",
    "\n",
    "`CalibratedClassifierCV` no es exclusivo para Naïve Bayes, puede utilizarse con otros algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Carga el fichero **heart_failure_clinical_records_dataset.csv** (es un archivo de texto). \n",
    "2. Separar el conjunto en 70% para entrenar y 30% para test (estratificado)\n",
    "3. Obtén la accuracy del `GaussianNB` con hiperparámetros por defecto.\n",
    "4. Haz una búsqueda de `var_smoothing` con `GridSearchCV()` utilizando los ejemplos del conjunto de entrenamiento y evalúa el mejor modelo con el conjunto de test.\n",
    "5. Realiza un calibrado de probabilidades y muestra las probabilidades obtenidas para los 5 primeros ejemplos\n",
    "\n",
    "Estos ejercicios no es necesario entregarlos."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
