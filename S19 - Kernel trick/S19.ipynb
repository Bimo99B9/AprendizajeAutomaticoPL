{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logo de AA1](logo_AA1_texto_small.png) \n",
    "# Sesión 19 - Kernel trick\n",
    "\n",
    "En la práctica anterior hemos trabajado con Máquinas de Vectores Soporte tratando de resolver los problemas buscando un hiperplano lineal capaz de separar las clases en el espacio de los atributos o espacio de entrada.\n",
    "\n",
    "Acabamos la práctica anterior mostrando un conjunto de datos que era imposible separar linealmente aunque pusiésemos valores de `C` muy altos:\n",
    "\n",
    "![No separable](fig_no_separable.png) \n",
    "\n",
    "En esta sesión práctica vamos a ver cómo se puede adaptar el SVM para obtener un buen rendimiento en estos conjuntos que no son separables linealmente en el espacio de entrada.\n",
    "\n",
    "Vamos, como siempre, a verlo a través de un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "def pinta(X_clase0, X_clase1, texto, espacio_entrada):\n",
    "    fig, ax = plt.subplots()\n",
    "    if espacio_entrada == True: \n",
    "        # muestra el espacio de entrada\n",
    "        ax.scatter(X_clase0[:, 0], X_clase0[:, 1], c='r', label='clase 0')\n",
    "        ax.scatter(X_clase1[:, 0], X_clase1[:, 1], c='b', label='clase 1')\n",
    "        ax.set_xlabel('x1')\n",
    "    else: \n",
    "        # muestra el espacio de características específico para este caso\n",
    "        ax.scatter(X_clase0[:, 0]**2, X_clase0[:, 1], c='r', label='clase 0')\n",
    "        ax.scatter(X_clase1[:, 0]**2, X_clase1[:, 1], c='b', label='clase 1')\n",
    "        ax.set_xlabel('x1^2')\n",
    "    ax.set_title(texto)\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n#####################################################\")\n",
    "print(\"#### Cargamos conjunto de datos\")\n",
    "print(\"#####################################################\")\n",
    "\n",
    "df = pd.read_excel('ejemplo.xlsx', sheet_name='datos1')\n",
    "filas, columnas = df.shape\n",
    "\n",
    "# separamos las primeras columnas y las almacenamos en X\n",
    "X = df.iloc[:,0:(columnas-1)]\n",
    "\n",
    "# separamos la clase\n",
    "y = df.iloc[:,(columnas-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código anterior hemos cargado los datos del ejemplo y hemos definido una función que utilizaremos para representar esos datos de dos formas.\n",
    "\n",
    "Vamos a visualizar los datos en el espacio de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinta(X[y==0].values, X[y==1].values, 'Espacio de entrada', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.1 Espacio de características\n",
    "\n",
    "Como podemos ver, es imposible trazar una recta que sea capaz de separar los ejemplos de ambas clases sin cometer fallos. Sin embargo, si obtenemos más atributos a partir de los que ya tenemos podríamos representar esos mismos ejemplos en otro espacio donde tal vez sí se puedan separar los ejemplos de manera sencilla. A ese nuevo espacio lo conocemos como **espacio de características** (*feature space*):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\phi \\colon \\mathbb{R}^d &\\to \\mathbb{R}^p\\\\\n",
    "  x &\\mapsto \\phi(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "La función $\\phi$ transforma los ejemplos del espacio de entrada al espacio de características. Así, a partir de un ejemplo $x$ formado por $x_1$ y $x_2$ obtendríamos su transformación al espacio de características $\\phi(x)$ que podría estar compuesto por características como $x_1^2$, $x_2^2$ o $x_1x_2$.\n",
    "\n",
    "Veamos qué pasa en el conjunto de datos anterior si en lugar de representar los ejemplos en el espacio de entrada, donde $x=(x_1,x_2)$, lo representamos en el espacio de características $\\phi(x)=(x_1^2,x_2)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinta(X[y==0].values, X[y==1].values, 'Espacio de característcas', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora todos seríamos capaces de trazar la recta que separa ambas clases y si a la SVM le presentásemos los ejemplos de esta manera, no tendría dificultad para encontrar el hiperplano que mejor separa las clases maximizando el margen.\n",
    "\n",
    "Lo que hemos hecho ha sido proyectar los ejemplos en un nuevo espacio donde es más sencillo separalos linealmente. Hemos transformado los ejemplos añadiéndoles atributos que se basan en los atributos que ya conocemos. Problemas:\n",
    "1. Normalmente no conoceremos cuál es la transformación necesaria o cuáles son las combinaciones entre los atributos necesarias para que el espacio de características sea útil.\n",
    "2. Si decidimos que queremos elevar al cuadrado y al cubo todos los atributos y que además queremos multiplicar unos atributos con otros para generar nuevos atributos, nos encontramos con que tenemos que realizar muchos cálculos y además es posible que el número de atributos sea tan grande que sea difícil almacenar todos los datos.\n",
    "\n",
    "Por ejemplo, si tenemos un conjunto en el que los ejemplos se definen con tres atributos $x=(x_1,x_2,x_3)$ y queremos calcular los atributos comentados anteriormente (sin calcular los cubos), nos encontraríamos con el siguiente espacio de características: $\\phi(x)=(x_1,x_2,x_3,x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3,x_1x_2^2,x_1x_3^2,x_2x_1^2,x_2x_3^2,x_3x_1^2,x_3x_2^2,x_1^2x_2^2,x_1^2x_3^2,x_2^2x_3^2)$. Hemos pasado de 3 atributos a 18 características. Ahora imaginad cómo podría ser el espacio de características si también incorporamos los cubos y si en lugar de haber 3 atributos en el espacio de entrada hubiese 10. El crecimiento es exponencial.\n",
    "\n",
    "## 19.2 Kernel trick (o truco del kernel)\n",
    "\n",
    "En la sesión anterior vimos que una SVM lineal aprende una función como:\n",
    "\n",
    "$$\n",
    "f(z)=w^T z + b = \\left( \\sum^n_{i=1} \\alpha^{(i)} y^{(i)} x^{(i)} \\right)^T z + b = \\sum^n_{i=1} \\alpha^{(i)} y^{(i)} \\langle x^{(i)}, z \\rangle + b \n",
    "$$\n",
    "\n",
    "donde $z$ es el ejemplo que se quiere valorar y $x^{(i)}$ es uno de los $n$ ejemplos del conjunto de entrenamiento. Si observamos la ecuación, vemos que lo que nos interesa es poder calcular el producto escalar de $z$ con los ejemplos de entrenamiento $\\langle x^{(i)}, z \\rangle$.\n",
    "\n",
    "El truco del kernel consiste en ser capaces de calcular el producto escalar entre $z$ y $x$ en el espacio de características sin proyectar ni $z$ ni $x$ en ese espacio. Y eso es lo que hace la función kernel $k\\left( x, z \\right)=\\langle \\phi(x), \\phi(z) \\rangle$:\n",
    "\n",
    "$$\n",
    "f(z)=w^T z + b = \\sum^n_{i=1} \\alpha^{(i)} y^{(i)} \\langle x^{(i)}, z \\rangle + b =  \\sum^n_{i=1} \\alpha^{(i)} y^{(i)} \\langle \\phi(x^{(i)}), \\phi(z) \\rangle + b = \\sum^n_{i=1} \\alpha^{(i)} y^{(i)} K\\left( x^{(i)}, z \\right) + b \n",
    "$$\n",
    "\n",
    "Por tanto, ahora necesitamos encontrar funciones kernel que nos permitan calcular el producto escalar de dos vectores en un espacio de características sin necesidad de calcular explícitamente esas características.\n",
    "\n",
    "### 19.2.1 Kernel lineal\n",
    "\n",
    "El kernel lineal es el kernel con el que estuvimos trabajando en la práctica anterior. \n",
    "\n",
    "En este caso, el espacio de entrada es igual al espacio de características $\\phi(x)=x$ y, por tanto, $K\\left( x, z \\right)=\\langle x, z \\rangle$.\n",
    "\n",
    "Para utilizar ester kernel es necesario utilizar el hiperparámetro `kernel='linear'`: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "### 19.2.2 Kernel polinómico\n",
    "\n",
    "Si tenemos que $x=(x_1,x_2)$ y creemos que la solución a nuestro problema puede obtenerse más fácilmente elevando los atributos al cuadrado y multiplicando unos atributos por otros, entonces nos gustaría tener un espacio de características en el que $\\phi(x)=(x_1,x_2,x_1^2,x_2^2,x_1x_2)$. \n",
    "\n",
    "El kernel polinómico calcula el producto escalar de dos vectores en un espacio de característacas similar al que acabamos de describir: https://en.wikipedia.org/wiki/Polynomial_kernel \n",
    "\n",
    "Para lograrlo realiza el siguiente cálculo: \n",
    "$$\n",
    "K(x,z)=\\left(\\langle x, z \\rangle + c \\right)^d\n",
    "$$\n",
    "\n",
    "Para utilizar este kernel en el `SVC` debemos tener en cuenta los siguientes hiperparámetros:\n",
    "- `kernel='poly'`\n",
    "- `degree` se refiere al grado $d$ al que se eleva la ecuación anterior\n",
    "- `coef0` se refiere al término $c$ de la ecuación anterior. Vamos a ver qué importancia tiene en el espacio de características.\n",
    "\n",
    "Imaginemos que queremos calcular $K(x,z)=\\left(\\langle x, z \\rangle + 0 \\right)^2$ siendo el espacio de entrada de 2 dimensiones:\n",
    "\n",
    "$$\n",
    "K(x,z)=\\left(\\langle x, z \\rangle\\right)^2 = \\left(x_1z_1 + x_2z_2\\right)^2 = x_1^2z_1^2 + x_2^2z_2^2 + 2x_1z_1x_2z_2 = \\langle \\left(x_1^2, x_2^2,\\sqrt{2}x_1x_2\\right), \\left(z_1^2, z_2^2,\\sqrt{2}z_1z_2\\right) \\rangle =\\langle \\phi(x), \\phi(z) \\rangle\n",
    "$$\n",
    "\n",
    "Como vemos $\\phi(x)=\\left(x_1^2, x_2^2,\\sqrt{2}x_1x_2\\right)$, con lo que hemos perdido los atributos originales. Veamos qué solución aportaría este kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una SVM con kernel polinómico de grado 2 con coef0=0\n",
    "sys_svc = SVC(kernel='poly', degree=2, coef0=0)\n",
    "\n",
    "# entrenamos\n",
    "sys_svc.fit(X,y)\n",
    "\n",
    "# se genera el display con el espacio coloreado\n",
    "disp = DecisionBoundaryDisplay.from_estimator(sys_svc, X, response_method=\"predict\", cmap=plt.cm.RdYlBu, alpha=0.6, xlabel=X.columns[0], ylabel=X.columns[1])\n",
    "\n",
    "# se incorporan los datos\n",
    "disp.ax_.scatter(X.values[:,0], X.values[:,1], c=y, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "disp.ax_.set_xlim([-1, 1])\n",
    "disp.ax_.set_ylim([-0.2, 0.8])\n",
    "disp.ax_.set_title('Superficie de cada clase en el espacio de entrada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solución es buena, pero no perfecta. Al principio de la práctica vimos que si utilizamos $(x_1^2,x_2)$ las clases eran fácilmente separables, así que al no contar con $x_2$ en el espacio de caractrísticas estamos dificultando la tarea de la SVM.\n",
    "\n",
    "Vamos a calcular ahora $K(x,z)=\\left(\\langle x, z \\rangle + 1 \\right)^2$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "K(x,z) \\left(x_1z_1 + x_2z_2 + 1\\right)^2 &= x_1^2z_1^2 + x_2^2z_2^2 + 1^2 + 2x_1z_1x_2z_2 + 2x_1z_11 + 2x_2z_21 \\\\\n",
    "&= \\langle \\left(x_1^2, x_2^2, 1, \\sqrt{2}x_1x_2, \\sqrt{2}x_1, \\sqrt{2}x_2 \\right), \\left(z_1^2, z_2^2, 1, \\sqrt{2}z_1z_2, \\sqrt{2}z_1, \\sqrt{2}z_2\\right) \\rangle =\\langle \\phi(x), \\phi(z) \\rangle\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Utilizando `coef0=1` ya no perdemos los atributos originales. Veamos la solución que obtenemos con este kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una SVM con kernel polinómico de grado 2 con coef0=1\n",
    "sys_svc = SVC(kernel='poly', degree=2, coef0=1)\n",
    "\n",
    "# entrenamos\n",
    "sys_svc.fit(X,y)\n",
    "\n",
    "# se genera el display con el espacio coloreado\n",
    "disp = DecisionBoundaryDisplay.from_estimator(sys_svc, X, response_method=\"predict\", cmap=plt.cm.RdYlBu, alpha=0.6, xlabel=X.columns[0], ylabel=X.columns[1])\n",
    "\n",
    "# se incorporan los datos\n",
    "disp.ax_.scatter(X.values[:,0], X.values[:,1], c=y, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "disp.ax_.set_xlim([-1, 1])\n",
    "disp.ax_.set_ylim([-0.2, 0.8])\n",
    "disp.ax_.set_title('Superficie de cada clase en el espacio de entrada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí obtenemos la mejor solución.\n",
    "\n",
    "Cuando utilizamos el kernel polinómico es habitual fijar `coef0=1` y probar varios valores para `degree`.\n",
    "\n",
    "### 19.2.3 Kernel RBF\n",
    "\n",
    "El kernel RBF (Radial Basis Function) es un kernel muy utilizado por su capacidad de adaptación a todo tipo de problemas. Se calcula mediante la siguiente fórmula:\n",
    "$$\n",
    "K(x,z)=\\exp\\left(- \\frac{\\lVert x - z \\rVert^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Si definimos $\\gamma = \\frac{1}{2\\sigma^2}$, entonces obtendremos la siguiente fórmula para refereirnos al kernel RBF:\n",
    "$$\n",
    "K(x,z)=\\exp\\left(-\\gamma\\lVert x - z \\rVert^2\\right)\n",
    "$$\n",
    "\n",
    "El espacio de características en el que se calcula el producto escalar es un espacio infinito que no puede ser calculado ni representado, pero sí se puede calcular su producto escalar utilizando este kernel.\n",
    "\n",
    "Para utilizar este kernel en el `SVC` debemos tener en cuenta los siguientes hiperparámetros:\n",
    "- `kernel='rbf'`\n",
    "- `gamma` debe ser un valor en el rango $(0,inf)$\n",
    "\n",
    "Veamos qué solución propone SMV con kernel RBF y `gamma` por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una SVM con kernel rbf con gamma por defecto\n",
    "sys_svc = SVC(kernel='rbf')\n",
    "\n",
    "# entrenamos\n",
    "sys_svc.fit(X,y)\n",
    "\n",
    "# se genera el display con el espacio coloreado\n",
    "disp = DecisionBoundaryDisplay.from_estimator(sys_svc, X, response_method=\"predict\", cmap=plt.cm.RdYlBu, alpha=0.6, xlabel=X.columns[0], ylabel=X.columns[1])\n",
    "\n",
    "# se incorporan los datos\n",
    "disp.ax_.scatter(X.values[:,0], X.values[:,1], c=y, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "disp.ax_.set_xlim([-1, 1])\n",
    "disp.ax_.set_ylim([-0.2, 0.8])\n",
    "disp.ax_.set_title('Superficie de cada clase en el espacio de entrada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solución es bastante buena y seguro que buscando valores óptimos para los hiperparámetros `gamma` y `C` podríamos alcanzar una mejor solución.\n",
    "\n",
    "Vamos ahora a proponerle una tarea un poco más complicada, donde los ejemplos de una de las clases se encuentren concentrados en dos zonas del espacio de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('ejemplo.xlsx', sheet_name='datos2')\n",
    "filas, columnas = df.shape\n",
    "\n",
    "# separamos las primeras columnas y las almacenamos en X\n",
    "X = df.iloc[:,0:(columnas-1)]\n",
    "\n",
    "# separamos la clase\n",
    "y = df.iloc[:,(columnas-1)]\n",
    "\n",
    "pinta(X[y==0].values, X[y==1].values, 'Espacio de entrada', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tarea no es sencilla ya que hay muchos más ejemplos de la clase 0 y los ejemplos de la clase 1 están separados en dos grupos.\n",
    "\n",
    "Vamos a ver qué sucede si variamos la `gamma`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una SVM con kernel rbf con gamma por defecto\n",
    "sys_svc = SVC(kernel='rbf')\n",
    "\n",
    "# creamos una figura con 4 gráficas\n",
    "fig, sub = plt.subplots(1, 4)\n",
    "fig.set_size_inches(16, 5)\n",
    "\n",
    "col = 0\n",
    "for g in [0.1, 1, 10, 100]:\n",
    "    sys_svc.set_params(gamma=g)  # cambiamos la gamma\n",
    "    sys_svc.fit(X,y)             # entrenamos\n",
    "\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(sys_svc, X, response_method=\"predict\", cmap=plt.cm.RdYlBu, \n",
    "        ax=sub[col], alpha=0.6, xlabel=X.columns[0], ylabel=X.columns[1])\n",
    "    sub[col].set_xlim([0, 1])\n",
    "    sub[col].set_ylim([0, 1])\n",
    "    sub[col].scatter(X.values[:,0], X.values[:,1], c=y, cmap=plt.cm.RdYlBu)\n",
    "    #sub[col].scatter(sys_svc.support_vectors_[:,0], sys_svc.support_vectors_[:,1], s=100, facecolors='none', edgecolors='k')\n",
    "    sub[col].set_title('Superficie con gamma='+str(g))\n",
    "    col = col + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuanto mayor sea el valor de `gamma` más se ajustará el modelo a los datos. Ahora debemos tener cuidado con los valores que utilicemos para `gamma` y para `C` porque corremos riesgo de sobreajustarnos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una SVM con kernel rbf con gamma por defecto\n",
    "sys_svc = SVC(kernel='rbf')\n",
    "\n",
    "# creamos una figura con 4 gráficas\n",
    "fig, sub = plt.subplots(3, 4)\n",
    "fig.set_size_inches(16, 14)\n",
    "\n",
    "fila = 0\n",
    "for c in [0.1, 1, 10]:\n",
    "    sys_svc.set_params(C=c)  # cambiamos la C\n",
    "    col = 0\n",
    "    for g in [1, 10, 100, 1000]:\n",
    "        sys_svc.set_params(gamma=g)  # cambiamos la gamma\n",
    "        sys_svc.fit(X,y)             # entrenamos\n",
    "\n",
    "        disp = DecisionBoundaryDisplay.from_estimator(sys_svc, X, response_method=\"predict\", cmap=plt.cm.RdYlBu, \n",
    "            ax=sub[fila][col], alpha=0.6, xlabel=X.columns[0], ylabel=X.columns[1])\n",
    "        sub[fila][col].set_xlim([0, 1])\n",
    "        sub[fila][col].set_ylim([0, 1])\n",
    "        sub[fila][col].scatter(X.values[:,0], X.values[:,1], c=y, cmap=plt.cm.RdYlBu)\n",
    "        #sub[fila][col].scatter(sys_svc.support_vectors_[:,0], sys_svc.support_vectors_[:,1], s=100, facecolors='none', edgecolors='k')\n",
    "        sub[fila][col].set_title('C='+str(c)+', gamma='+str(g))\n",
    "        col = col + 1\n",
    "    fila = fila + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que valores altos de `C` y `gamma` pueden llevarnos a soluciones que claramente están sobreajustadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.2.4 Otros kernels\n",
    "\n",
    "Los kernels lineal, polinómico y RBF son los más utilizados.\n",
    "\n",
    "Sin embargo, existen otros kernels o incluso tenemos la posibilidad de crear nosotros nuestros propios kernels:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/svm.html#svm-kernels \n",
    "- https://en.wikipedia.org/wiki/String_kernel\n",
    "- https://en.wikipedia.org/wiki/Fisher_kernel\n",
    "- https://en.wikipedia.org/wiki/Kernel_smoother\n",
    "- ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Carga el fichero **ionosphere.data** (es un archivo de texto). \n",
    "2. Prueba los kernels polinómico y RBF (prueba diferentes valores para los hiperparámetros `C`, `degree` y `gamma`).\n",
    "3. Busca los mejores valores para los hiperparámetros y evalúa el rendimiento final del modelo (utiliza una `GridSearchCV` dentro de una validación cruzada como se vio en la sesión 11). \n",
    "\n",
    "Estos ejercicios no es necesario entregarlos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
