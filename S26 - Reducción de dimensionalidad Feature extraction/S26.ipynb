{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logo de AA1](logo_AA1_texto_small.png) \n",
    "# Sesión 26 - Reducción de Dimensionalidad: Feature Extraction\n",
    "\n",
    "Ya habíamos comentado en sesión anterior que la **Reducción de la Dimensionalidad** consiste en reducir el número de atributos obteniendo o creando un subconjunto de atributos o características que resuman la información relevante. \n",
    "\n",
    "Vimos que la Reducción de la Dimensionalidad puede lograrse por dos vías:\n",
    "1. **Feature Selection** (Selección de Características), donde nos quedaremos con los atributos o características más relevantes del conjunto original.\n",
    "2. **Feature Extraction** (Extracción de Características), donde crearemos nuevos atributos mediante la combinación de los atributos originales.\n",
    "\n",
    "En esta sesión nos centraremos en la *Extracción de Características*.\n",
    "\n",
    "Para ello vamos a utilizar un conjunto de datos que también trata de identificar dígitos a partir a partir de imágenes, pero en este caso, en lugar de partir de un conjunto de atributos reducido, trabajaremos con los atributos originales, los pixeles. Las imágenes son de tamaño $28 \\times 28$, así que cada imagen tendrá un total de 784 pixeles y cada uno de ellos tendrá un valor entre 0 y 255 que indicará la intensidad de gris.\n",
    "\n",
    "Se trata de un problema de clasificación donde hay que generar un modelo capaz de predecir el dígito al que corresponde la imagen. Hay 10 clases posibles que se corresponden con los 10 dígitos que hay entre el 0 y el 9.\n",
    "\n",
    "En este caso, el conjunto de datos ya viene separado en entrenamiento y test (*mnist_train.pkl* y *mnist_test.pkl*) conteniendo cada uno de los ficheros un `DataFrame` con los datos necesarios. Cada fila del dataframe contiene 785 datos, que se corresponden con los 784 pixeles de una imagen y la clase (dígito) correspondiente a la imagen, que se sitúa en la última columna. Estos ficheros están codificados en formato `pickle`, que es un formato muy utilizado para *serializar* objetos, es decir codificar un objeto en un formato que nos permita transportarlo: https://es.wikipedia.org/wiki/Serialización \n",
    "\n",
    "Desde la librería `Pandas` es muy sencillo trabajar con el formato `pickle` puesto que disponemos de las funciones `to_pickle()` y `read_pickle()`:\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html?highlight=to_pickle#pandas.DataFrame.to_pickle\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.read_pickle.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### cargar el conjunto de entrenamiento')\n",
    "print('##########################################')\n",
    "\n",
    "# leemos el conjunto de entrenamiento desde un fichero pkl\n",
    "df_train = pd.read_pickle('mnist_train.pkl')\n",
    "filas, columnas = df_train.shape\n",
    "\n",
    "# la clase está en la última columna \n",
    "# separamos los atributos y los almacenamos en X\n",
    "X_train = df_train.iloc[:,0:(columnas-1)]\n",
    "\n",
    "# separamos la clase y la almacenamos en Y\n",
    "y_train = df_train.iloc[:,(columnas-1)]\n",
    "\n",
    "print('Número de ejemplos: ', X_train.shape[0])\n",
    "print('Número de atributos:', X_train.shape[1])\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### cargar el conjunto de test')\n",
    "print('##########################################')\n",
    "\n",
    "# leemos el conjunto de test desde un fichero pkl\n",
    "df_test = pd.read_pickle('mnist_test.pkl')\n",
    "filas, columnas = df_test.shape\n",
    "\n",
    "# la clase está en la última columna \n",
    "# separamos los atributos y los almacenamos en X\n",
    "X_test = df_test.iloc[:,0:(columnas-1)]\n",
    "\n",
    "# separamos la clase y la almacenamos en Y\n",
    "y_test = df_test.iloc[:,(columnas-1)]\n",
    "\n",
    "print('Número de ejemplos: ', X_test.shape[0])\n",
    "print('Número de atributos:', X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como veis, este conjunto de datos tiene muchos ejemplos y muchos atributos, así que los tiempos de entrenamiento pueden empezar a ser ya un poco más largos. Por ejemplo, el k-vecinos ya empezaría a tener dificultades puesto que para clasificar un solo ejemplo necesitaría calcular su distancia respecto a los 60000 ejemplos de conjunto de entrenamiento.\n",
    "\n",
    "Veamos el rendimiento de un árbol de clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un árbol, entrenamos y evaluamos\n",
    "sys_tree = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "sys_tree.fit(X_train, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred = sys_tree.predict(X_test)\n",
    "acc_tree =  metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy árbol = %.4f\" % acc_tree)\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos obtener un rendimiento bastante más alto utilizando Random Forest, pero el tiempo de ejecición también sería superior. Para lo que queremos ver en esta sesión práctica continuaremos utilizando `DecisionTreeClassifier`.\n",
    "\n",
    "Aunque los ejemplos ya nos vienen dados en forma de vector, comentamos al principio de la práctica que se correspondían con imágenes. Vamos a visualizar algunas de esas imágenes convirtiendo esos vectores en matrices de $28 \\times 28$ y mostrándolos como imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### visualizar ejemplos')\n",
    "print('##########################################')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(18):\n",
    "    plt.subplot(3, 6, i+1)\n",
    "    idx = randint(0, X_train.shape[0]) # seleccionamos una imagen al azar y la mostramos\n",
    "    plt.imshow(X_train.values[idx].reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos hecho que las imágenes se vayan eligiendo de forma aleatoria, así que si vuelves a ejecutar el código anterior se mostrarán otras 18 imágenes diferentes.\n",
    "\n",
    "## 26.1 Análisis de Componentes Principales\n",
    "\n",
    "El Análisis de Componentes Principales o PCA es el método más polular de extracción de características.\n",
    "\n",
    "PCA trata de encontrar un nuevo conjunto de ejes ortogonales en el que la varianza de los datos sea máxima en alguna dirección. Realiza el proceso de manera secuencial, es decir:\n",
    "1. busca una combinación lineal de los atributos que maximiza la varianza en alguna dirección (esta sería la primera componente principal o PC1)\n",
    "2. se busca la siguiente componente principal, que debe ser perpendicular a la anterior y debe maximizar la varianza en alguna de las direcciones posibles\n",
    "3. se repite el paso 2\n",
    "\n",
    "El Análisis de Componentes Principales **no utiliza la clase** en su análisis, se trata de un sistema no supervisado.\n",
    "\n",
    "No vamos a entrar en detalles teóricos puesto que el Análisis de Componentes Principales se explica en detalle en la asignatura \"Análisis de Datos\" de segundo curso y segundo cuatrimestre.\n",
    "\n",
    "Suele utilizarse con dos fines:\n",
    "- obtener un conjunto de atributos más manejable\n",
    "- visualizar los datos en un espacio de dimensiones reducido\n",
    "\n",
    "### 26.1.1 Obtener un número de atributos manejable\n",
    "\n",
    "El primero de los objetivos es reducir la dimensionalidad para facilitar posteriores procesos de aprendizaje.\n",
    "\n",
    "Vamos a ver qué ocurre si a partir de los 784 atributos iniciales buscamos únicamente 20 componentes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### PCA')\n",
    "print('##########################################')\n",
    "\n",
    "num_componentes = 20\n",
    "pca = PCA(n_components=num_componentes)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "pca.fit(X_train)  # OJO, no necesita la clase para entrenar\n",
    "fin = time.time()\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido las 20 componentes (en muy poco tiempo) y hemos proyectado `X_train` en el espacio de esas 20 componentes almacenando el resultado en `X_train_pca`. \n",
    "\n",
    "Fijaros en que al realizar la llamada a `fit()` no es necesario pasarle `y_train` ya que el Análisis de Componentes Principales no utiliza la clase en su análisis.\n",
    "\n",
    "Vamos a ver cuál será el rendimiento de un árbol de decisión en este nuevo espacio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### Rendimiento con PCA -', num_componentes, 'componentes')\n",
    "print('##########################################')\n",
    "\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# creamos un árbol, entrenamos y evaluamos\n",
    "sys_tree = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "sys_tree.fit(X_train_pca, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred = sys_tree.predict(X_test_pca)\n",
    "acc_tree =  metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy árbol = %.4f\" % acc_tree)\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rendimiento es un poco peor, pero vemos que el árbol a tardado muchísimo menos en entrenarse.\n",
    "\n",
    "A veces, estas reducciones de tiempo son la diferencia entre que un sistema sea útil o no.\n",
    "\n",
    "Si aumentásemos el número de componentes principales iríamos mejorando el rendimiento y empeorando en tiempo de ejecución.\n",
    "\n",
    "Veamos lo que aporta cada componente principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### Gráfica varianza - ', num_componentes, 'componentes')\n",
    "print('##########################################')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Gráfico para ver la varianza explicada por cada componente principal\n",
    "ax.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "\n",
    "# Gráfico para ver la varianza explicada acumulada\n",
    "ax.plot(range(1,len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_), \n",
    "                c='red', label='Varianza explicada acumulada')\n",
    " \n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('Número de componentes principales')\n",
    "ax.set_ylabel('Varianza explicada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el atributo `explained_variance_ratio` podemos ver la varianza explicada por cada una de las componentes principales y la varianza explicada que acumulan todas ellas.\n",
    "\n",
    "Utilizando 20 componentes vemos que podemos explicar algo más del 60% de la varianza en los datos y que la primera componente explica casi el 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26.1.2 Visualizar los datos\n",
    "\n",
    "Otro de los usos importantes del Análisis de Componentes Principales es la visualización de los datos.\n",
    "\n",
    "Si trabajamos únicamente con las 2 primeras componentes podremos representar los datos en un gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### PCA - 2 componentes')\n",
    "print('##########################################')\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "pca.fit(X_train)  # OJO, no necesita la clase para entrenar\n",
    "fin = time.time()\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Visualizar los datos')\n",
    "print('##########################################')\n",
    "\n",
    "cuantos = 100\n",
    "[max_cp1, max_cp2] = X_train_pca[:cuantos].max(axis=0)\n",
    "[min_cp1, min_cp2] = X_train_pca[:cuantos].min(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(cuantos): \n",
    "    ax.text(X_train_pca[i, 0], X_train_pca[i, 1], str(y_train[i]), color=plt.cm.Set1(y_train[i])) \n",
    "ax.set_xlim([min_cp1, max_cp1+100])\n",
    "ax.set_ylim([min_cp2, max_cp2+100])\n",
    "plt.show()\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Rendimiento con PCA - 2 componentes')\n",
    "print('##########################################')\n",
    "\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# creamos un árbol, entrenamos y evaluamos\n",
    "sys_tree = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "sys_tree.fit(X_train_pca, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred = sys_tree.predict(X_test_pca)\n",
    "acc_tree =  metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy árbol = %.4f\" % acc_tree)\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se han mostrado solo 100 ejemplos para que se pueda visualizar con mayor claridad.\n",
    "\n",
    "Podemos apreciar en la gráfica cómo estas dos componentes ya son capaces de diferenciar algunas de las clases.\n",
    "\n",
    "Evidentemente, el resultado que se obtiene con un árbol de decisión utilizando estas dos variables es muy pobre, algo que se podría preveer ya que estas dos primeras componentes apenas explican el 18% de la varianza.\n",
    "\n",
    "Sin embargo, veréis que esta es una técnica muy utilizada para visualizar los datos.\n",
    "\n",
    "Podemos también utilizar las 3 primeras componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### PCA - 3 componentes')\n",
    "print('##########################################')\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "pca.fit(X_train)  # OJO, no necesita la clase para entrenar\n",
    "fin = time.time()\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Visualizar los datos')\n",
    "print('##########################################')\n",
    "\n",
    "cuantos = 100\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X_train_pca[:cuantos,0], X_train_pca[:cuantos,1], X_train_pca[:cuantos,2], c=y_train[:cuantos], cmap=plt.cm.Set1)\n",
    "plt.show()\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Rendimiento con PCA - 3 componentes')\n",
    "print('##########################################')\n",
    "\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# creamos un árbol, entrenamos y evaluamos\n",
    "sys_tree = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "sys_tree.fit(X_train_pca, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred = sys_tree.predict(X_test_pca)\n",
    "acc_tree =  metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy árbol = %.4f\" % acc_tree)\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados mejoran un poco y todavía se pueden visualizar los datos. Si implementáis esto mismo en un script o programa podréis rotar el gráfico para verlo desde el ángulo que queráis y veréis cómo las clases están algo más diferenciadas.\n",
    "\n",
    "## 26.2 Análisis Discriminante Lineal\n",
    "\n",
    "El Análisis Discriminante Lineal (LDA) también busca combinaciones lineales de los atributos para obtener nuevos atributos con mayor riqueza explicativa. \n",
    "\n",
    "LDA parte con cierta ventaja puesto que **se apoya en la clase para el cálculo de los nuevos atributos**. Su implementación en `Scikit-learn` tiene el nombre de `LinearDiscriminantAnalysis` y tiene un funcionamiento muy similar al de `PCA`. \n",
    "\n",
    "En LDA el número máximo de componentes que se puede calcular es igual al número de clases del problema menos 1.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n",
    "No vamos a entrar en detalles teóricos puesto que, al igual que ocurre con el Análisis de Componente Principales, el LDA se explica en detalle en la asignatura \"Análisis de Datos\" de segundo curso y segundo cuatrimestre.\n",
    "\n",
    "Fijáos que en el `fit()` debemos indicar la clase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### LinearDiscriminantAnalysis - 2 componentes')\n",
    "print('##########################################')\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "lda.fit(X_train, y_train)  # OJO, necesita la clase\n",
    "fin = time.time()\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))\n",
    "\n",
    "X_train_lda = lda.transform(X_train)\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Visualizar los datos')\n",
    "print('##########################################')\n",
    "\n",
    "cuantos = 100\n",
    "[max_cp1, max_cp2] = X_train_lda[:cuantos].max(axis=0)\n",
    "[min_cp1, min_cp2] = X_train_lda[:cuantos].min(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(cuantos): \n",
    "    ax.text(X_train_lda[i, 0], X_train_lda[i, 1], str(y_train[i]), color=plt.cm.Set1(y_train[i])) \n",
    "ax.set_xlim([min_cp1, max_cp1])\n",
    "ax.set_ylim([min_cp2, max_cp2])\n",
    "plt.show()\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Rendimiento con LinearDiscriminantAnalysis - 2 componentes')\n",
    "print('##########################################')\n",
    "\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# creamos un árbol, entrenamos y evaluamos\n",
    "sys_tree = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "sys_tree.fit(X_train_lda, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred = sys_tree.predict(X_test_lda)\n",
    "acc_tree =  metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy árbol = %.4f\" % acc_tree)\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código es equivalente al visto con `PCA`.\n",
    "\n",
    "Vemos que ha tardado un poco más en calcular las componentes, sin embargo, podemos apreciar en el gráfico que ha separado un poco mejor las clases y eso facilita el trabajo del árbol de decisión, que tarda menos en entrenarse y obtiene un resultado mejor.\n",
    "\n",
    "Podemos ver qué sucede utilizando 3 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### LinearDiscriminantAnalysis - 3 componentes')\n",
    "print('##########################################')\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "lda.fit(X_train, y_train)  # OJO, necesita la clase\n",
    "fin = time.time()\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))\n",
    "\n",
    "X_train_lda = lda.transform(X_train)\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Visualizar los datos')\n",
    "print('##########################################')\n",
    "\n",
    "cuantos = 100\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X_train_lda[:cuantos,0], X_train_lda[:cuantos,1], X_train_lda[:cuantos,2], c=y_train[:cuantos], cmap=plt.cm.Set1)\n",
    "plt.show()\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Rendimiento con LinearDiscriminantAnalysis - 3 componentes')\n",
    "print('##########################################')\n",
    "\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# creamos un árbol, entrenamos y evaluamos\n",
    "sys_tree = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "sys_tree.fit(X_train_lda, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred = sys_tree.predict(X_test_lda)\n",
    "acc_tree =  metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy árbol = %.4f\" % acc_tree)\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos mejora el rendimiento considerablemente y en la figura podemos apreciar las clases algo más separadas.\n",
    "\n",
    "Vamos a probar con distinto número de componentes (recuerda que con 10 clases LDA puede calcular como mucho 9 componentes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##########################################')\n",
    "print('### LinearDiscriminantAnalysis')\n",
    "print('##########################################')\n",
    "\n",
    "num_componentes = 9\n",
    "lda = LinearDiscriminantAnalysis(n_components=num_componentes)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "lda.fit(X_train, y_train)  # OJO, necesita la clase\n",
    "fin = time.time()\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))\n",
    "\n",
    "X_train_lda = lda.transform(X_train)\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Gráfica varianza - ', num_componentes, 'componentes')\n",
    "print('##########################################')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Gráfico para ver la varianza explicada por cada componente principal\n",
    "ax.bar(range(1,len(lda.explained_variance_ratio_)+1), lda.explained_variance_ratio_)\n",
    "\n",
    "# Gráfico para ver la varianza explicada acumulada\n",
    "ax.plot(range(1,len(lda.explained_variance_ratio_)+1), np.cumsum(lda.explained_variance_ratio_), \n",
    "                c='red', label='Varianza explicada acumulada')\n",
    " \n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('Número de componentes')\n",
    "ax.set_ylabel('Varianza explicada')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\n##########################################')\n",
    "print('### Rendimiento con LinearDiscriminantAnalysis - ', num_componentes, 'componentes')\n",
    "print('##########################################')\n",
    "\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# creamos un árbol, entrenamos y evaluamos\n",
    "sys_tree = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "# se toman tiempos de ejecución \n",
    "ini = time.time()\n",
    "sys_tree.fit(X_train_lda, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred = sys_tree.predict(X_test_lda)\n",
    "acc_tree =  metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy árbol = %.4f\" % acc_tree)\n",
    "print(\"Tiempo: %.2f segundos\" % (fin-ini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que las 3 primeras componentes ya explican en torno al 60% de la varianza. \n",
    "\n",
    "Utilizando el máximo número de componentes se explica el 100% de la varianza. El árbol que se entrena con estas 9 componentes es rápido en su aprendizaje y además tiene una accuracy cercana a la que se obtiene utilizando todos los atributos originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Carga el conjunto de datos  **gastroenterology.data** (OJO, los ejemplos vienen colocados en columnas, no en filas, y el número de ejemplos y atributos no coincide con lo indicado en en names)\n",
    "2. Sepáralo en 80% para entrenar y 20% para test.\n",
    "3. Prueba los algoritmos `PCA` y `LinearDiscriminantAnalysis` \n",
    "\n",
    "Estos ejercicios no es necesario entregarlos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
