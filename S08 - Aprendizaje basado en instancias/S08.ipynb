{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logo de AA1](logo_AA1_texto_small.png) \n",
    "# Sesión 08 - Aprendizaje basado en instancias\n",
    "\n",
    "En el aprendizaje basado en instancias (Instance Based Learning) los algoritmos memorizan los ejemplos o instancias que han visto en el entrenamiento. Por tanto, la fase de entrenamiento es muy simple, consiste únicamente en almacenar los ejemplos para utilizarlos, posteriormente, en la fase de predicción.\n",
    "\n",
    "La fase de predicción es la que asume toda la carga computacional, puesto que cuando se le presente un nuevo ejemplo y se le pida que lo clasifique, deberá, de alguna manera, comparar con los ejemplos almacenados y tomará una decisión respecto a la clase a predecir.\n",
    "\n",
    "## 8.1 K-Nearest Neighbor (KNN)\n",
    "\n",
    "El algoritmo emblemático de este tipo de sistemas es el conocido como *el vecino más próximo*, *K-Nearest Neighbor* (KNN) y consiste, básicamente, en buscar la instancia más cercana al ejemplo que queremos clasificar de entre las que se han memorizado durante el entrenamiento. La clase que se predecirá para el ejemplo será la misma que tenga su instancia más cercana. \n",
    "\n",
    "Normalmente, para evitar situaciones en las que por culpa de una instancia mal situada (ruido) se clasifiquen mal algunos ejemplos, en lugar de utilizar únicamente el vecino más cercano, se seleccionan los `K` vecinos más cercanos y luego se hace una votación entre ellos para decidir la clase a predecir.\n",
    "\n",
    "Veámoslo con un ejemplo. Para ello vamos a cargar los datos que hay en la pestaña 'ejemplo' del fichero 'ejemplo.xlsx'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df = pd.read_excel('ejemplo.xlsx', sheet_name='ejemplo')\n",
    "filas, columnas = df.shape\n",
    "\n",
    "# separamos las primeras columnas y las almacenamos en X\n",
    "X = df.iloc[:,0:(columnas-1)]\n",
    "display(X)\n",
    "\n",
    "# separamos la clase\n",
    "y = df.iloc[:,(columnas-1)]\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trata de un conjunto de datos muy simple en el que los ejemplos vienen descritos por dos atributos (x1 y x2) y en el que los 5 primeros ejemplos pertenecen a la clase '0' y los otros 5 a la clase '1'.\n",
    "\n",
    "Vamos a representarlos utilizando la función `scatter()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinta(X_clase0, X_clase1, texto):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X_clase0[:, 0], X_clase0[:, 1], c='r', label='clase 0')\n",
    "    ax.scatter(X_clase1[:, 0], X_clase1[:, 1], c='b', label='clase 1')\n",
    "    ax.set_title(texto)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "pinta(X[:5].values, X[5:].values, 'Ejemplos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos apreciar que en la parte izquierda se sitúan los ejemplos de una clase y en la derecha los de la otra. \n",
    "\n",
    "Vamos a ver ahora cómo se crea una instancia de la clase `KNeighborsClassifier()`, donde le indicamos que queremos utilizar los 3 vecinos más cercanos (`n_neighbors=3`) para decidir la clase a predecir. Luego entrenamos el sistema, que lo que hará será simplemente almacenar las instancias del conjunto de entrenamiento: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una instancia del KNN para 3 vecinos\n",
    "knn_sis = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# entrenamos el sistema\n",
    "knn_sis.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 Obtener lo vecinos más cercanos\n",
    "Vamos ahora a ver cuáles son las instancias más cercanas para un nuevo ejemplo que está situado en la posición [7, 3] (x1=7 y x2=3) utilizando el método `kneighbors()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_ejemplo = [7, 3]\n",
    "\n",
    "# va entre corchetes porque podríamos darle varios ejemplos\n",
    "nuevo_conjunto = pd.DataFrame(data=[nuevo_ejemplo], columns=['x1', 'x2'])\n",
    "\n",
    "(distancias, indices) = knn_sis.kneighbors(nuevo_conjunto)\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\"Datos de los 3 vecinos más cercanos:\")\n",
    "# el [0] es porque solo queremos ver el resultado del primer ejemplo (solo hay uno)\n",
    "print(\"Distancias:\", distancias[0]) \n",
    "print(\"Índices de las instancias:\", indices[0])\n",
    "print(\"Instancias:\")\n",
    "display(X.iloc[indices[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `kneighbors()` devuelve las distancias y los índices de las instancias más cercanas. Utilizando `iloc[]` podemos identificar fácilmente las instancias implicadas y representarlas:\n",
    "\n",
    "![3 más cercanos](fig_Ejemplos3cercanos.png)\n",
    "\n",
    "### 8.1.2 Prediciendo la clase y la probabilidad de cada clase\n",
    "\n",
    "En la imagen vemos representado con un punto verde el nuevo ejemplo y resaltadas las 3 instancias más cercanas.\n",
    "\n",
    "Vemos que 2 de las 3 instancias más cercanas son de la clase '1', así que al pedir una predicción para este ejemplo, la predicción será '1':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict(nuevo_conjunto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos algoritmos implementados en el `scikit-learn` son capaces de calcular la probabilidad de pertenencia del ejemplo a cada una de las clases.\n",
    "\n",
    "Cuando esto es así, se puede utilizar el método `predict_proba()`, que devolverá un vector de probabilidades dadas en el orden indicado por la propiedad `classes_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict_proba(nuevo_conjunto))\n",
    "print('Orden de las clases:', knn_sis.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, ¿qué pasa cuando se produce un empate?. Cuando tras realizar la votación las dos clases reciben el mismo número de votos entoces lo que suele hacer este algoritmo es predecir una de las clases al azar o predecir la primera de ellas según el orden que aparece en `classes_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos el número de vecinos utilizando 'set_params()'\n",
    "knn_sis.set_params(n_neighbors=4)\n",
    "print(knn_sis.get_params())\n",
    "knn_sis.fit(X, y)\n",
    "\n",
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict(nuevo_conjunto))\n",
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict_proba(nuevo_conjunto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de crear un nuevo sistema `KNeighborsClassifier()` con 4 vecinos, lo que hemos hecho es utilizar el método `set_params()` para modificar el número de vecinos que el sistema utilizará para calcular las predicciones.\n",
    "\n",
    "![4 más cercanos](fig_Ejemplos4cercanos.png)\n",
    "\n",
    "Con 4 vecinos resulta que 2 son de una clase y 2 de la otra, de tal forma que la probabilidad de que el ejemplo sea de la clase '0' es 0.5 (al igual que de la clase '1').\n",
    "\n",
    "### 8.1.3 Distancia ponderada\n",
    "\n",
    "Pero en la figura vemos que no todos los vecinos más próximos están a la misma distancia y esto es algo de lo que se puede sacar provecho si hacemos que el voto de las instancias más cercanas tengan más peso que el de las instancias más lejanas.\n",
    "\n",
    "Para lograr esto, tenemos que cambiar nuevamente los hiper-parámetros del sistema indicando `weights='distance'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos a distancia ponderada utilizando 'set_params()'\n",
    "knn_sis.set_params(weights='distance')\n",
    "print(knn_sis.get_params())\n",
    "knn_sis.fit(X, y)\n",
    "\n",
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict(nuevo_conjunto))\n",
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict_proba(nuevo_conjunto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha cambiado la predicción para ese ejemplo puesto que uno de los 4 vecinos más cercanos está un poco más alejado que los otros 3 y, por tanto, en la votación ponderada gana la clase '1'.\n",
    "\n",
    "### 8.1.4 La importancia de la distancia\n",
    "\n",
    "Como se puede apreciar, la distancia tiene suma importancia en este algoritmo y, aunque lo normal es utilizar la distancia euclídea, podrían utilizarse otras distancias utilizando los parámetros `metric` y `p`:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html \n",
    "\n",
    "Como ya comentamos en una práctica anterior, los algoritmos que trabajan con distancias tienen problemas cuando se les presentan conjuntos de datos en los que los atributos están presentados con órdenenes de magnitud diferentes. Para ilustrar este problema vamos a cargar los datos que están en la pestaña 'datos' del fichero 'ejemplo.xlsx':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('ejemplo.xlsx', sheet_name='datos')\n",
    "filas, columnas = df.shape\n",
    "\n",
    "# separamos las primeras columnas y las almacenamos en X\n",
    "X = df.iloc[:,0:(columnas-1)]\n",
    "\n",
    "# separamos la clase\n",
    "y = df.iloc[:,(columnas-1)]\n",
    "\n",
    "pinta(X[:50].values, X[50:].values, 'Conjunto de datos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el atributo `x1` varía en valores muy cercanos a 1 mientras que el atributo `x2` varía en valores entre 0 y 50. Esto implica que a la hora de calcular las distancias el atributo `x2` tiene mucho más peso que el atributo `x1`.\n",
    "\n",
    "En la figura vemos que los ejemplos de la clase '0' tienen en `x1` un valor por debajo de 1 y los de la clase '1' un valor mayor de 1. \n",
    "Si situásemos un ejemplo en las coordenadas [1.08, 5] vemos que se encontraría claramente situado en la zona de influencia de la clase '1' (parece estar rodeado de ejemplos de la clase '1').\n",
    "\n",
    "Sin embargo, como los valores en los que se mueve `x1` son muy pequeños con respecto a los que se toman para el atributo `x2` lo que realmente influye en el cálculo de los vecinos más próximos es el atributo `x2`. Se aprecia mejor la problemática que estamos comentando si vemos las instancias representadas en un gráfico en el que ambos ejes presenten la misma escala:\n",
    "\n",
    "![Misma escala en los dos ejes](fig_Conjunto_de_datos2.png)\n",
    "\n",
    "Visto de esta manera, ya no está tan claro que un ejemplo en las coordenadas [1.08, 5] sea de la clase '1'. Vamos a ver qué sucede si queremos clasificar ese ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una instancia del KNN\n",
    "num_vecinos = 5\n",
    "knn_sis = KNeighborsClassifier(n_neighbors=num_vecinos)\n",
    "\n",
    "# entrenamos\n",
    "knn_sis.fit(X, y)\n",
    "\n",
    "nuevo_ejemplo = [1.08, 5]\n",
    "\n",
    "# va entre corchetes porque podríamos darle varios ejemplos\n",
    "nuevo_conjunto = pd.DataFrame(data=[nuevo_ejemplo], columns=['x1', 'x2'])\n",
    "\n",
    "# obtenemos los vecinos\n",
    "(distancias, indices) = knn_sis.kneighbors(nuevo_conjunto)\n",
    "\n",
    "print(\"Datos de los\", num_vecinos, \"vecinos más cercanos:\")\n",
    "# el [0] es porque solo queremos ver el resultado del primer ejemplo (solo hay uno)\n",
    "print(\"Distancias:\", distancias[0]) \n",
    "print(\"Índices de las instancias:\", indices[0])\n",
    "print(\"Instancias:\")\n",
    "display(X.iloc[indices[0]])\n",
    "print(\"Clases de las instancias:\")\n",
    "display(y.iloc[indices[0]])\n",
    "\n",
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict(nuevo_conjunto))\n",
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict_proba(nuevo_conjunto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulta que sus ejemplos más cercanos son todos de la clase '0'. Prueba a ejecutar el código anterior aumentando el número de vecinos y verás que necesitas aumentarlo mucho para empezar a encontrar algún vecino de la clase '1'.\n",
    "\n",
    "Esta es una consecuencia de que los atributos estén en diferentes escalas.\n",
    "\n",
    "Calculemos el rendimiento del `KNN` en esta situación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a fijar el número de vecinos a 5\n",
    "knn_sis.set_params(n_neighbors=5)\n",
    "\n",
    "# entrenamos y evaluamos\n",
    "knn_sis.fit(X, y)\n",
    "y_pred = knn_sis.predict(X)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision :\", metrics.precision_score(y, y_pred)) \n",
    "print(\"Recall :\", metrics.recall_score(y, y_pred)) \n",
    "print(\"F1 :\", metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falla un 13% de los ejemplos, que son principalmente los de la parte baja de la clase '1'.\n",
    "\n",
    "Vamos ahora a estandarizar los atributos para que se muevan en valores similares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# entrenamos y trasnformamos los ejemplos de X\n",
    "X_std = standardizer.fit_transform(X)\n",
    "\n",
    "pinta(X_std[:50], X_std[50:], 'Conjunto estandarizado')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la apariencia es muy semejante a la que teníamos originalmente, pero ahora las escalas de ambos atributos son similares.\n",
    "\n",
    "Veamos dónde quedaría situado el ejemplo [1.08, 5] tras aplicarle la estandarización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_ejemplo = [1.08, 5]\n",
    "\n",
    "# va entre corchetes porque podríamos darle varios ejemplos\n",
    "nuevo_conjunto = pd.DataFrame(data=[nuevo_ejemplo], columns=['x1', 'x2'])\n",
    "\n",
    "# lo estandarizamos\n",
    "nuevo_conjunto_std = standardizer.transform(nuevo_conjunto)\n",
    "print(nuevo_conjunto_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tratáis de situarlo en el gráfico anterior veréis que, como no podría ser de otra manera, su posición es la que habíamos visto en un principio, estando totalmente rodeado de ejemplos de la clase '1'.\n",
    "\n",
    "Vamos ahora a entrenar un sistema con estos datos estandarizados y vamos a ver cómo se clasifica este ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una instancia del KNN\n",
    "num_vecinos = 5\n",
    "knn_sis = KNeighborsClassifier(n_neighbors=num_vecinos)\n",
    "\n",
    "# OJO: entrenamos con los datos estandarizados\n",
    "knn_sis.fit(X_std, y)\n",
    "\n",
    "# OJO: obtenemos los vecinos del ejemplo estandarizado\n",
    "(distancias, indices) = knn_sis.kneighbors(nuevo_conjunto_std)\n",
    "\n",
    "print(\"Datos de los\", num_vecinos, \"vecinos más cercanos:\")\n",
    "# el [0] es porque solo queremos ver el resultado del primer ejemplo (solo hay uno)\n",
    "print(\"Distancias:\", distancias[0]) \n",
    "print(\"Índices de las instancias:\", indices[0])\n",
    "print(\"Instancias estanzadizadas:\")\n",
    "display(X_std[indices[0]])\n",
    "print(\"Clases de las instancias:\")\n",
    "display(y.iloc[indices[0]])\n",
    "\n",
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict(nuevo_conjunto_std))\n",
    "print('Predicción para', nuevo_ejemplo, ':', knn_sis.predict_proba(nuevo_conjunto_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podéis ver, ahora sus vecinos sí son de la clase '1'.\n",
    "\n",
    "Vamos a ver el rendimiento del `KNN` utilizando el conjunto estandarizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a fijar el número de vecinos a 5\n",
    "knn_sis.set_params(n_neighbors=5)\n",
    "\n",
    "# entrenamos y evaluamos utilizando los ejemplos estandarizados X_std\n",
    "knn_sis.fit(X_std, y)\n",
    "y_pred = knn_sis.predict(X_std)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision :\", metrics.precision_score(y, y_pred)) \n",
    "print(\"Recall :\", metrics.recall_score(y, y_pred)) \n",
    "print(\"F1 :\", metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el sistema acierta el 98% de los ejemplos (sin estandarizar se acertaba el 87%).\n",
    "\n",
    "**¿Quiere esto decir que hay que estandarizar siempre? No.** Esto quiere decir que al realizar transformaciones en el conjunto de datos estaremos modificando la relación de los atributos entre sí. Esto puede resultar beneficioso a veces, pero en otras ocasiones puede ser perjudicial. También depende del algoritmo que estemos utilizando. Acabamos de ver que el k-vecinos es un algoritmo que se puede ver muy afectado (normalmente beneficiado) por una transformación de este tipo, sin embargo, otros algoritmos pueden no verse afectados en absoluto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8.1.5 Creación de `pipelines`\n",
    "\n",
    "En el código anterior hemos realizado primero un preprocesado de los datos (`X_std = standardizer.fit_transform(X)`) y luego hemos utilizado los datos para entrenar el sistema (`knn_sis.fit(X_std, y)`) y para realizar la predicción (`y_pred = knn_sis.predict(X_std)`).\n",
    "\n",
    "Ya vimos que si quisiésemos evaluar un nuevo ejemplo, deberíamos primero estandarizarlo (utilizando `transform()` en lugar de `fit_transform()` puesto que el `standarizer` ya se entrenó con el conjunto de datos) y luego realizar la predicción:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_ejemplo = [1.08, 20]\n",
    "print('Ejemplo:', nuevo_ejemplo)\n",
    "\n",
    "# va entre corchetes porque podríamos darle varios ejemplos\n",
    "nuevo_conjunto = pd.DataFrame(data=[nuevo_ejemplo], columns=['x1', 'x2'])\n",
    "\n",
    "# lo transformamos (na hay que hacer fit)\n",
    "ejemplo_estandarizado = standardizer.transform(nuevo_conjunto)\n",
    "print('Ejemplo estandarizado:', ejemplo_estandarizado)\n",
    "\n",
    "# realizamos la predicción\n",
    "prediccion = knn_sis.predict(ejemplo_estandarizado)\n",
    "print('Clase:', prediccion[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sería más práctico que pudiésemos crear un sistema al que le diésemos los ejemplos sin procesar y él mismo se encargase de procesar los datos y luego realizar el entreniento o la predicción.\n",
    "\n",
    "Eso podemos lograrlo mediante el uso de `Pipelines`: https://scikit-learn.org/stable/modules/compose.html#pipeline\n",
    "\n",
    "Vamos a ver cómo se haría en este caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea un pipeline que primero estandariza y después enlaza con un KNN\n",
    "std_knn = Pipeline([('std', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
    "\n",
    "print('Pasos del pipeline:', std_knn.steps)\n",
    "print('Acceso mediante índice:', std_knn[0])\n",
    "print('Acceso mediante nombre:', std_knn['knn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un `pipeline` debemos indicar en un array los pasos que se darán mediante tuplas `('nombre_que_le_damos', algoritmo)`.\n",
    "\n",
    "En el código hemos creado un `pipeline` con una estandarización que llamamos 'std' y un `KNN` que llamamos 'knn'. Podemos utilizar el campo `steps`, para visualizar los pasos o acceder a ellos mediante índices o el nombre.\n",
    "\n",
    "También podemos modificar los hiper-parámetros de los algoritmos contenidos en el pipeline. Como hay varios algoritmos implicados, será imprescindible indicarle al método `set_params()` el nombre del algoritmo y el del parámetro que queremos modificar y para ello se sigue la siguiente sintaxis: `nombreAlgoritmo__nombreParámetro`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos el número de vecinos al algoritmo que nombremos como 'knn'\n",
    "std_knn.set_params(knn__n_neighbors=1)\n",
    "\n",
    "print('Parámetros:', std_knn.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora ya podemos entrenar y realizar predicciones directamente desde el conjunto `X` que no está estandarizado (ya estandarizará el sistema `std_knn` que hemos creado con un `pipeline`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora se le pasan los ejemplos sin estandarizar porque ya lo hace él\n",
    "std_knn.fit(X, y) \n",
    "\n",
    "# lo mismo para realizar las predicciones\n",
    "y_pred = std_knn.predict(X)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision :\", metrics.precision_score(y, y_pred)) \n",
    "print(\"Recall :\", metrics.recall_score(y, y_pred)) \n",
    "print(\"F1 :\", metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede que hayas detectado una limitación a lo que acabamos de ver: **hemos tratado todos los atributos por igual**.\n",
    "\n",
    "A veces unos atributos necesitarán ser preprocesados de una manera y otros atributos de otra manera. Una caso típico es que los atributos categóricos necesitarán ser transformados mediante una codificación one-hot mientras que los atributos numéricos podrían necesitar una estandarización.\n",
    "\n",
    "Dedicaremos una práctica a final de curso a ver cómo incluir tranformaciones diferentes de los atributos dentro de un mismo `pipeline`.\n",
    "\n",
    "### 8.1.6 La importancia del número de vecinos\n",
    "\n",
    "En el ejemplo anterior, puede llamarnos la atención que se realiza una predicción perfecta: acierta todos los ejemplos cuando ponemos un solo vecino. Está muy bien, pero **tiene truco**: hemos entrenado con el conjunto `X` y hemos realizado la predicción sobre los mismos ejemplos, así que como utilizamos un sólo vecino para predecir, ¡¡el vecino más próximo de cada ejemplo será el propio ejemplo!! Así es imposible fallar ;-)\n",
    "\n",
    "En la próxima práctica veremos cómo realizar una experimentación de manera correcta (sin truco) para que el rendimiento que obtengamos sea representativo de cómo va a funcionar de bien (o mal) nuestro sistema cuando esté en producción.\n",
    "\n",
    "Veamos qué pasa cuando modificamos el número de vecinos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    print(\"\\n#####################################################\")\n",
    "    print(\"#### n_neighbors =\", i)\n",
    "   \n",
    "    std_knn.set_params(knn__n_neighbors=i)\n",
    "    std_knn.fit(X, y)\n",
    "    y_pred = std_knn.predict(X)\n",
    "    \n",
    "    print(\"Accuracy :\", metrics.accuracy_score(y, y_pred))\n",
    "    print(\"(P=%.2f ,R=%.2f ,F1=%.2f)\" % (metrics.precision_score(y, y_pred), metrics.recall_score(y, y_pred), metrics.f1_score(y, y_pred))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los resultados varían. \n",
    "\n",
    "**No existe un número de vecinos mágico que funcione mejor en todas las situaciones y con todos los conjuntos.**\n",
    "\n",
    "El número de vecinos es un **hiperparámetro** del algoritmo, es decir, es un mecanismo que tenemos para hacer funcionar al algoritmo de forma ligeramente diferente y que tendremos que ajustar nosotros. Veremos cómo se ajusta en una sesión de prácticas posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Carga el fichero **ILPD.data** (es un archivo de texto). \n",
    "2. Convierte los atributos categóricos y asigna valores a los missing si los hay.\n",
    "3. Calcula el error con el baseline de \"la clase más frecuente\".\n",
    "4. Calcula la accuracy para un K-vecinos (KNN) variando el número de vecinos desde 1 hasta 10.\n",
    "5. Crea un pipeline estandarizando y con un k-vecinos y calcula la accuracy variando el número de vecinos desde 1 hasta 10.\n",
    "6. Regresenta en una gráfica la evolución de la accuracy en función del número de vecinos.\n",
    "\n",
    "Estos ejercicios no es necesario entregarlos."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
